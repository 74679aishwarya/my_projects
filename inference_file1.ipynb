{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "mLAXdqpezltE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/model.py')\n",
        "\n",
        "# Import the Seq2SeqTransformer class from the model.py file\n",
        "from model import Seq2SeqTransformer,config"
      ],
      "metadata": {
        "id": "x0CBabWLovB9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from tokenizers import Tokenizer\n",
        "import torch\n",
        "import transformers\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# Assume Seq2SeqTransformer and other necessary code is defined above\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = Tokenizer.from_file('/content/en_hi_mr_pa_te_ka_ml_or_gu_ur_ta_bn_as (1).json')\n",
        "\n",
        "special_tokens = {\n",
        "    '<unk>': 0,\n",
        "    '<pad>': 1,\n",
        "    '<s-en>': 2,  # bos english\n",
        "    '<s-hi>': 3,  # bos hindi\n",
        "    '<s-mr>': 4, # bos telugu\n",
        "    '<s-pa>': 5, # bos punjabi\n",
        "    '<s-te>': 6, # bos telugu\n",
        "    '<s-ka>': 7, # bos kannada\n",
        "    '<s-ml>': 8, # bos malayalam\n",
        "    '<s-or>': 9, # bos orish\n",
        "    '<s-gu>': 10, # bos gujarati\n",
        "    '<s-ur>': 11, # bos urdu\n",
        "    '<s-ta>': 12, # bos tamil\n",
        "    '<s-bn>': 13, # bos bengali\n",
        "    '<s-as>': 14, # bos asami\n",
        "\n",
        "    # Add other languages here...\n",
        "    '</s>': 15,  # eos\n",
        "}\n",
        "\n",
        "model = Seq2SeqTransformer(config)\n",
        "\n",
        "state_dict = torch.load('/content/best_model.pt', map_location='cpu')\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Define the translate function\n",
        "def translate(input_sentence, language, d=True, t=1.0):\n",
        "    # Additional checks and setup code...\n",
        "\n",
        "    input_ids = f\"<s-en>{input_sentence.strip()}</s>\"\n",
        "    input_ids = tokenizer.encode(input_ids).ids\n",
        "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0)\n",
        "    bos = special_tokens[f\"<s-{language}>\"]\n",
        "    outputs = model.generate(input_ids, deterministic=d, bos=bos, temperature=t)\n",
        "    translation = tokenizer.decode(outputs.numpy())\n",
        "    return translation\n",
        "\n",
        "# Directly set parameters for demonstration\n",
        "input_sentence =input(\"Enter the sentence:\")\n",
        "language = input(\"Enter language of your choice:\") # Example: 'hi' for Hindi\n",
        "d = True  # Deterministic\n",
        "t = 1.0  # Temperature\n",
        "\n",
        "# Call the translate function with directly set parameters\n",
        "translation = translate(input_sentence=input_sentence, language=language, d=d, t=t)\n",
        "print(translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClIehd7Qzod1",
        "outputId": "a3962686-ec28-47bd-d0ab-36c5c30509d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence:what is your name\n",
            "Enter language of your choice:hi\n",
            "आपका नाम क्या है\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu\n"
      ],
      "metadata": {
        "id": "oFO-banTzoh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc7f181-f110-47d1-e4c2-41a723fe5880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "import sacrebleu\n",
        "from collections import namedtuple\n",
        "\n",
        "# Assume your Seq2SeqTransformer, and other necessary code is defined here...\n",
        "\n",
        "# Load your model and tokenizer\n",
        "tokenizer = Tokenizer.from_file('/content/en_hi_mr_pa_te_ka_ml_or_gu_ur_ta_bn_as (1).json')\n",
        "# Initialize your model here...\n",
        "\n",
        "# Assuming you have a dataset of source sentences and their correct translations\n",
        "source_sentences = [\"This is a test.\", \"Another test.\", \"Yet another test.\"]\n",
        "target_sentences = [\"यह एक परीक्षण है।\", \"एक और परीक्षण।\", \"एक और परीक्षण।\"]  # Example in French\n",
        "\n",
        "# Define a function to calculate BLEU score\n",
        "def calculate_bleu(model, sources, targets, tokenizer, special_tokens):\n",
        "    predictions = []\n",
        "    for sentence in sources:\n",
        "        input_ids = f\"<s-en>{sentence.strip()}</s>\"\n",
        "        input_ids = tokenizer.encode(input_ids).ids\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0)\n",
        "        # Assuming <s-fr> is the token for French, change according to your target language\n",
        "        bos = special_tokens[f\"<s-{language}>\"]\n",
        "        # Replace generate function based on your model's API\n",
        "        output_ids = model.generate(input_ids, bos=bos)\n",
        "        prediction = tokenizer.decode(output_ids.numpy(), skip_special_tokens=True)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    bleu_score = sacrebleu.corpus_bleu(predictions, [targets]).score\n",
        "    return bleu_score\n",
        "\n",
        "# Calculate the BLEU score\n",
        "bleu_score = calculate_bleu(model, source_sentences, target_sentences, tokenizer, special_tokens)\n",
        "print(f\"BLEU score: {bleu_score}\")\n"
      ],
      "metadata": {
        "id": "9GHF7RKWzlKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7833374-ce0a-4849-8bb9-86296dcfde28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 3.21858262703621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "import pytesseract\n",
        "\n",
        "# Example for Windows\n",
        "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SfHX9H5W4gB",
        "outputId": "9e28b86a-419e-41b4-f63d-caba1644256e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCiWR2U4TcDr",
        "outputId": "4834987c-8b3d-405d-8183-90e6d7559c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Example predictions and references\n",
        "predictions = [\"आप अगले 5 वर्षों में आप को कैसे देख रहे हैं\", \"तुम कहाँ जाना चाहते हैं\"]\n",
        "references = [\"आप अगले 5 वर्षों में खुद को कहां देखते हैं\", \"आप कहाँ जाना चाहते हैं\"]\n",
        "\n",
        "# Initialize the Rouge metric\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate scores\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "\n",
        "print(\"ROUGE-1:\", scores['rouge-1']['f'])\n",
        "print(\"ROUGE-2:\", scores['rouge-2']['f'])\n",
        "print(\"ROUGE-L:\", scores['rouge-l']['f'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbnij8-MTcIx",
        "outputId": "877d3e4b-033d-4a46-cd54-7d2180ae2106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1: 0.749999995\n",
            "ROUGE-2: 0.5855263107963989\n",
            "ROUGE-L: 0.749999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "0AbXEAJse51k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f7e82e-f1bb-4c42-a3a7-1dae70cc0be7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "# Assuming `test_data` is a list of tuples (input_sentence, reference_translation)\n",
        "hypotheses = [\"आप अगले 5 वर्षों में आप को कैसे देख रहे हैं\", \"तुम कहाँ जाना चाहते हैं\"]\n",
        "references = [\"आप अगले 5 वर्षों में खुद को कहां देखते हैं\", \"आप कहाँ जाना चाहते हैं\"]\n",
        "def calculate_bleu(hypotheses,references):\n",
        "    # Preprocess sentences\n",
        "    hypotheses = [sentence.split() for sentence in hypotheses]  # Tokenize hypothesis\n",
        "    references = [[ref.split()] for ref in references]  # Tokenize references and wrap each in a list\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    return corpus_bleu(references, hypotheses, smoothing_function=smoothing)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `translations` is a list of sentences generated by your model\n",
        "# and `actual_sentences` is a list of reference sentences\n",
        "bleu_score = calculate_bleu(hypotheses, references)\n",
        "print(f\"Calculated BLEU score: {bleu_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "0Nqk7L6te548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3d75fb-03b3-424f-a06f-539353509858"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated BLEU score: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAmO44ede58D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhKOA2AJe5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eaDZQmTAdSCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}